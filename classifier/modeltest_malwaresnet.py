import numpy as np
import common
import os
labels = common.fetch_samples()

from sklearn.model_selection import train_test_split
np.random.seed(123)
y_train, y_test, sha256_train, sha256_test = train_test_split(
    list(labels.values()), list(labels.keys()), test_size=1000)

##################
# end-to-end model
# maximum number of bytes we allow per file = 256KB, power of two for faster evaluation on endpoint
max_file_length = int(2**18)
file_chunks = 16  # break file into this many chunks
file_chunk_size = max_file_length // file_chunks
batch_size = 4

import malwaresnet
import math
from keras.callbacks import LearningRateScheduler

model_malwaresnet = malwaresnet.create_model(input_shape=(file_chunks, file_chunk_size), byte_embedding_size=2)
train_generator = common.generator(list(zip(sha256_train, y_train)), batch_size, file_chunks, file_chunk_size)
test_generator = common.generator(list(zip(sha256_test, y_test)), 1, file_chunks, file_chunk_size)
model_malwaresnet.fit_generator(train_generator,
                                steps_per_epoch=math.ceil(len(sha256_train) / batch_size),
                                epochs=20,
                                callbacks=[
                                    LearningRateScheduler(
                                        lambda epoch: common.schedule(epoch, start=0.1, decay=0.5, every=1)
                                        )
                                    ],
                                validation_data=test_generator,
                                validation_steps=len(sha256_test))
y_pred = []
for sha256, lab in zip(sha256_test, y_test):
    y_pred.append(
        model_malwaresnet.predict_on_batch(
            np.asarray([get_file_data(sha256, lab)]).reshape(
                (-1, file_chunks, file_chunk_size))
        )
    )
common.summarize_performance(np.asarray(y_pred).flatten(), y_test, "MalwaResnet")
model_malwaresnet.save('malwaresnet.h5')
